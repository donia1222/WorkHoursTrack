import * as FileSystem from 'expo-file-system';
import { SupportedLanguage } from '../contexts/LanguageContext';
import { VisionPrompts } from './VisionPrompts';

export interface ImageAnalysisResult {
  text?: string;
  labels?: {
    description: string;
    score: number;
  }[];
  objects?: {
    name: string;
    score: number;
  }[];
  error?: string;
}

export class GoogleVisionService {
  // API Keys de Google desde variables de entorno
  private static readonly GEMINI_API_KEY = process.env.EXPO_PUBLIC_GOOGLE_GEMINI_API_KEY || '';
  private static readonly GEMINI_API_URL = 'https://generativelanguage.googleapis.com/v1beta/models';
  private static readonly VISION_API_URL = 'https://vision.googleapis.com/v1/images:annotate';

  // Helper method to detect if the message is asking for specific person analysis
  private static isSpecificPersonAnalysis(userMessage: string): boolean {
    const specificAnalysisPatterns = [
      'Extrae los horarios espec√≠ficamente de',     // Spanish old
      'extrae √öNICAMENTE los horarios',            // Spanish new
      'Extract schedules specifically from',        // English old
      'extract ONLY the schedules',                // English new
      'Extrahieren Sie die Arbeitszeiten speziell von', // German
      'Extrayez les horaires sp√©cifiquement de',   // French
      'Estrai gli orari specificamente da',        // Italian
      'persona:',                                   // Generic pattern
      'person:'                                     // Generic pattern English
    ];
    
    return specificAnalysisPatterns.some(pattern => userMessage.toLowerCase().includes(pattern.toLowerCase()));
  }

  // Helper method to extract person name from the message
  private static extractPersonName(userMessage: string): string {
    // Pattern matching for different languages
    const patterns = [
      /Extrae los horarios espec√≠ficamente de "([^"]+)"/,     // Spanish old
      /de la persona:\s*([^.]+)/i,                             // Spanish new
      /Extract schedules specifically from "([^"]+)"/,        // English old
      /for the person:\s*([^.]+)/i,                           // English new
      /person:\s*([^.]+)/i,                                    // Generic
      /persona:\s*([^.]+)/i,                                   // Generic Spanish
      /Extrahieren Sie die Arbeitszeiten speziell von "([^"]+)"/, // German
      /Extrayez les horaires sp√©cifiquement de "([^"]+)"/,   // French
      /Estrai gli orari specificamente da "([^"]+)"/         // Italian
    ];
    
    for (const pattern of patterns) {
      const match = userMessage.match(pattern);
      if (match && match[1]) {
        return match[1];
      }
    }
    
    // Fallback: try to extract name after common words
    const fallbackPatterns = [
      /de "([^"]+)"/,  // Spanish/French fallback
      /from "([^"]+)"/, // English fallback
      /von "([^"]+)"/, // German fallback
      /da "([^"]+)"/   // Italian fallback
    ];
    
    for (const pattern of fallbackPatterns) {
      const match = userMessage.match(pattern);
      if (match && match[1]) {
        return match[1];
      }
    }
    
    return '';
  }

  static async analyzeImage(imageUri: string, features: string[] = ['TEXT_DETECTION', 'LABEL_DETECTION'], language: SupportedLanguage = 'en'): Promise<ImageAnalysisResult> {
    try {
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠

      // Convertir imagen a base64
      const base64Image = await FileSystem.readAsStringAsync(imageUri, {
        encoding: FileSystem.EncodingType.Base64,
      });

      // Preparar request para Google Vision API
      const requestBody = {
        requests: [
          {
            image: {
              content: base64Image,
            },
            features: features.map(feature => ({
              type: feature,
              maxResults: 10,
            })),
          },
        ],
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google no configurada');
      }

      const apiUrl = `${this.VISION_API_URL}?key=${this.GEMINI_API_KEY}`;
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        throw new Error(`Error en la API: ${response.status}`);
      }

      const data = await response.json();
      
      if (data.responses?.[0]?.error) {
        throw new Error(data.responses[0].error.message);
      }

      const result: ImageAnalysisResult = {};
      const annotations = data.responses?.[0];

      // Extraer texto detectado
      if (annotations?.textAnnotations?.length > 0) {
        result.text = annotations.textAnnotations[0].description;
      }

      // Extraer etiquetas/labels
      if (annotations?.labelAnnotations?.length > 0) {
        result.labels = annotations.labelAnnotations.map((label: any) => ({
          description: label.description,
          score: label.score,
        }));
      }

      // Extraer objetos detectados
      if (annotations?.localizedObjectAnnotations?.length > 0) {
        result.objects = annotations.localizedObjectAnnotations.map((obj: any) => ({
          name: obj.name,
          score: obj.score,
        }));
      }

      return result;
    } catch (error) {
      console.error('Error analizando imagen:', error);
      return {
        error: error instanceof Error ? error.message : 'Error desconocido',
      };
    }
  }

  static async extractTextOnly(imageUri: string, language: SupportedLanguage = 'en'): Promise<string> {
    const result = await this.analyzeImage(imageUri, ['TEXT_DETECTION'], language);
    return result.text || result.error || VisionPrompts.getPrompt('noTextDetected', language);
  }

  static async getImageDescription(imageUri: string, language: SupportedLanguage = 'en'): Promise<string> {
    console.log('üîç [VISION] Iniciando an√°lisis completo de imagen...');
    
    // Hacer an√°lisis completo: OCR + detecci√≥n de objetos + etiquetas
    const result = await this.analyzeImage(imageUri, ['TEXT_DETECTION', 'LABEL_DETECTION', 'OBJECT_LOCALIZATION'], language);
    
    if (result.error) {
      console.error('‚ùå [VISION] Error en an√°lisis:', result.error);
      return `Error: ${result.error}`;
    }

    let description = VisionPrompts.getImageAnalysisTitle(language);
    
    // PRIMERO: Texto extra√≠do (lo m√°s importante para planes de trabajo)
    if (result.text && result.text.trim()) {
      console.log('üìù [VISION] Texto extra√≠do:', result.text);
      description += VisionPrompts.getTextDetectedTitle(language);
      description += `${result.text}\n\n`;
    } else {
      console.log('‚ö†Ô∏è [VISION] No se detect√≥ texto legible');
      description += VisionPrompts.getNoTextWarning(language);
    }
    
    // SEGUNDO: Elementos generales (contexto adicional)
    if (result.labels && result.labels.length > 0) {
      description += VisionPrompts.getElementsDetectedTitle(language);
      result.labels
        .filter(label => label.score > 0.5)
        .slice(0, 5)
        .forEach(label => {
          const confidence = Math.round(label.score * 100);
          description += `‚Ä¢ ${label.description} (${confidence}% seguro)\n`;
        });
      description += '\n';
    }

    // TERCERO: Objetos espec√≠ficos
    if (result.objects && result.objects.length > 0) {
      description += VisionPrompts.getSpecificObjectsTitle(language);
      result.objects
        .filter(obj => obj.score > 0.5)
        .slice(0, 3)
        .forEach(obj => {
          const confidence = Math.round(obj.score * 100);
          description += `‚Ä¢ ${obj.name} (${confidence}% seguro)\n`;
        });
    }

    console.log('‚úÖ [VISION] Descripci√≥n generada:', description);
    return description;
  }

  // Funci√≥n para respuestas conversacionales con Gemini
  static async getChatResponse(message: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('ü§ñ [GEMINI] Iniciando getChatResponse...');
      console.log('üìù [GEMINI] Mensaje recibido:', message.substring(0, 200) + '...');
      
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠
      console.log('‚úÖ [GEMINI] Usando proxy seguro');

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: `${VisionPrompts.getChatAssistantPrompt(language)}

Usuario: ${message}`
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        },
        safetySettings: [
          {
            category: "HARM_CATEGORY_HARASSMENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_HATE_SPEECH",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          }
        ]
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google Gemini no configurada');
      }

      const apiUrl = `${this.GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${this.GEMINI_API_KEY}`;
      console.log('üì§ [GEMINI] Enviando request directo a Google Gemini API...');
      console.log('üì¶ [GEMINI] Request body:', JSON.stringify(requestBody, null, 2));

      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      console.log('üì° [GEMINI] Response status:', response.status);
      console.log('üì° [GEMINI] Response headers:', Object.fromEntries(response.headers.entries()));

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå [GEMINI] Error completo:', errorData);
        throw new Error(`Error en Gemini API: ${response.status} - ${errorData.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      console.log('üì• [GEMINI] Response data completa:', JSON.stringify(data, null, 2));

      if (data.candidates && data.candidates.length > 0) {
        const candidate = data.candidates[0];
        console.log('üéØ [GEMINI] Candidate encontrado:', candidate);
        
        if (candidate.content && candidate.content.parts && candidate.content.parts.length > 0) {
          const responseText = candidate.content.parts[0].text;
          console.log('‚úÖ [GEMINI] Respuesta exitosa:', responseText);
          return responseText;
        } else {
          console.warn('‚ö†Ô∏è [GEMINI] Candidate sin contenido v√°lido');
        }
      } else {
        console.warn('‚ö†Ô∏è [GEMINI] No hay candidates en la respuesta');
      }

      console.log('üîÑ [GEMINI] Retornando respuesta por defecto');
      return VisionPrompts.getErrorMessage('noResponse', language);
    } catch (error) {
      console.error('Error en Gemini API:', error);
      return VisionPrompts.getErrorMessage('processing', language);
    }
  }

  // Funci√≥n para analizar imagen directamente con Gemini Vision
  static async analyzeImageWithGeminiVision(imageUri: string, userMessage: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('üëÅÔ∏è [GEMINI-VISION] Iniciando an√°lisis con Gemini Vision...');
      console.log('üì± [GEMINI-VISION] Image URI:', imageUri);
      console.log('üí¨ [GEMINI-VISION] User message:', userMessage);
      
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠

      // Convertir imagen a base64
      const base64Image = await FileSystem.readAsStringAsync(imageUri, {
        encoding: FileSystem.EncodingType.Base64,
      });
      console.log('üñºÔ∏è [GEMINI-VISION] Imagen convertida a base64');

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: this.isSpecificPersonAnalysis(userMessage)
                  ? VisionPrompts.buildSpecificPersonPrompt(language, userMessage, this.extractPersonName(userMessage))
                  : VisionPrompts.buildGeneralAnalysisPrompt(language, userMessage)
              },
              {
                inline_data: {
                  mime_type: "image/jpeg",
                  data: base64Image
                }
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        }
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google Gemini no configurada');
      }

      const apiUrl = `${this.GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${this.GEMINI_API_KEY}`;
      console.log('üì§ [GEMINI-VISION] Enviando request directo a Google Gemini API...');
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      console.log('üì° [GEMINI-VISION] Response status:', response.status);

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå [GEMINI-VISION] Error:', errorData);
        throw new Error(`Error en Gemini Vision: ${response.status} - ${errorData.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      console.log('üì• [GEMINI-VISION] Response data:', JSON.stringify(data, null, 2));

      if (data.candidates && data.candidates.length > 0) {
        const candidate = data.candidates[0];
        if (candidate.content && candidate.content.parts && candidate.content.parts.length > 0) {
          const responseText = candidate.content.parts[0].text;
          console.log('‚úÖ [GEMINI-VISION] Respuesta exitosa:', responseText);
          return responseText;
        }
      }

      console.log('‚ö†Ô∏è [GEMINI-VISION] Respuesta vac√≠a, usando fallback');
      return VisionPrompts.getErrorMessage('visionAnalysis', language);

    } catch (error) {
      console.error('‚ùå [GEMINI-VISION] Error:', error);
      return VisionPrompts.getErrorMessage('geminiVision', language);
    }
  }

  // Funci√≥n para respuesta con contexto conversacional
  static async getChatResponseWithContext(message: string, conversationHistory: any[], currentImage?: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('üß† [CONTEXTO] Iniciando respuesta con contexto conversacional...');
      console.log('üìö [CONTEXTO] Historial de conversaci√≥n:', conversationHistory);
      console.log('üñºÔ∏è [CONTEXTO] Imagen actual:', currentImage ? 'S√≠' : 'No');
      
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠

      // Crear contexto de conversaci√≥n usando VisionPrompts
      const contextPrompt = VisionPrompts.buildContextPrompt(
        language,
        message,
        conversationHistory,
        !!currentImage
      );

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: contextPrompt
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        },
        safetySettings: [
          {
            category: "HARM_CATEGORY_HARASSMENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_HATE_SPEECH",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          },
          {
            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold: "BLOCK_MEDIUM_AND_ABOVE"
          }
        ]
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google Gemini no configurada');
      }

      const apiUrl = `${this.GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${this.GEMINI_API_KEY}`;
      console.log('üì§ [CONTEXTO] Enviando request directo a Google Gemini API...');
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå [CONTEXTO] Error:', errorData);
        throw new Error(`Error en Gemini API: ${response.status} - ${errorData.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      console.log('üì• [CONTEXTO] Response data:', JSON.stringify(data, null, 2));

      if (data.candidates && data.candidates.length > 0) {
        const candidate = data.candidates[0];
        if (candidate.content && candidate.content.parts && candidate.content.parts.length > 0) {
          const responseText = candidate.content.parts[0].text;
          console.log('‚úÖ [CONTEXTO] Respuesta con contexto exitosa:', responseText);
          return responseText;
        }
      }

      return VisionPrompts.getErrorMessage('noResponse', language);
    } catch (error) {
      console.error('‚ùå [CONTEXTO] Error:', error);
      return VisionPrompts.getErrorMessage('processing', language);
    }
  }

  // Funci√≥n para analizar documentos PDF
  static async analyzePDFDocument(documentUri: string, documentName: string, userMessage: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('üìÑ [PDF] Iniciando an√°lisis de documento PDF...');
      console.log('üìÅ [PDF] Document URI:', documentUri);
      console.log('üìã [PDF] Document name:', documentName);
      console.log('üí¨ [PDF] User message:', userMessage);
      
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠

      // Convertir PDF a base64
      const base64Document = await FileSystem.readAsStringAsync(documentUri, {
        encoding: FileSystem.EncodingType.Base64,
      });
      console.log('üìÑ [PDF] Documento convertido a base64');

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: this.isSpecificPersonAnalysis(userMessage)
                  ? VisionPrompts.buildSpecificPersonPrompt(language, userMessage, this.extractPersonName(userMessage))
                  : VisionPrompts.buildGeneralAnalysisPrompt(language, userMessage || `Analiza este documento PDF llamado "${documentName}"`)
              },
              {
                inline_data: {
                  mime_type: "application/pdf",
                  data: base64Document
                }
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        }
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google Gemini no configurada');
      }

      const apiUrl = `${this.GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${this.GEMINI_API_KEY}`;
      console.log('üì§ [PDF] Enviando request directo a Google Gemini API...');
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      console.log('üì° [PDF] Response status:', response.status);

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå [PDF] Error:', errorData);
        throw new Error(`Error en an√°lisis de PDF: ${response.status} - ${errorData.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      console.log('üì• [PDF] Response data:', JSON.stringify(data, null, 2));

      if (data.candidates && data.candidates.length > 0) {
        const candidate = data.candidates[0];
        if (candidate.content && candidate.content.parts && candidate.content.parts.length > 0) {
          const responseText = candidate.content.parts[0].text;
          console.log('‚úÖ [PDF] Respuesta exitosa:', responseText);
          return responseText;
        }
      }

      console.log('‚ö†Ô∏è [PDF] Respuesta vac√≠a, usando fallback');
      return `He recibido el documento PDF "${documentName}" pero no pude procesarlo completamente. Los PDFs a veces requieren procesamiento especial. ¬øPodr√≠as intentar convertirlo a imagen o enviar una captura de pantalla del plan de trabajo?`;

    } catch (error) {
      console.error('‚ùå [PDF] Error:', error);
      return `Lo siento, hubo un problema al analizar el documento PDF "${documentName}". Los PDFs pueden requerir procesamiento especial. Como alternativa, puedes tomar una captura de pantalla del documento y enviarla como imagen.`;
    }
  }

  // Funci√≥n h√≠brida que usa an√°lisis interactivo primero (solo espa√±ol), luego Gemini Vision, y Vision API como fallback
  static async analyzeWorkPlan(imageUri: string, userMessage: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('üîÄ [H√çBRIDO] Iniciando an√°lisis h√≠brido de plan de trabajo...');
      
      // M√âTODO 1: Intentar con an√°lisis interactivo de horarios (solo espa√±ol por ahora)
      if (language === 'es') {
        console.log('ü§ñ [INTERACTIVO] Usando an√°lisis interactivo de horarios...');
        try {
          const interactiveResult = await this.analyzeWorkPlanInteractive(imageUri, userMessage, language);
          if (interactiveResult) {
            console.log('‚úÖ [INTERACTIVO] An√°lisis interactivo exitoso!');
            return interactiveResult;
          }
        } catch (error) {
          console.log('‚ö†Ô∏è [INTERACTIVO] An√°lisis interactivo fall√≥, usando m√©todo normal...');
          console.error('üîç [INTERACTIVO] Error:', error);
        }
      }
      
      // M√âTODO 2: Intentar con Gemini Vision primero (m√°s inteligente)
      console.log('üëÅÔ∏è [H√çBRIDO] Probando con Gemini Vision...');
      try {
        const geminiResult = await this.analyzeImageWithGeminiVision(imageUri, userMessage, language);
        
        // Si Gemini Vision funcion√≥ y no devolvi√≥ un mensaje de error
        if (geminiResult && !geminiResult.includes(VisionPrompts.getErrorMessage('geminiVision', language))) {
          console.log('‚úÖ [H√çBRIDO] Gemini Vision exitoso!');
          return geminiResult;
        }
      } catch (error) {
        console.log('‚ö†Ô∏è [H√çBRIDO] Gemini Vision fall√≥, usando Vision API...');
        console.error('üîç [H√çBRIDO] Error Gemini Vision:', error);
      }

      // M√âTODO 3: Fallback a Vision API + Gemini texto
      console.log('üîÑ [H√çBRIDO] Usando Vision API + Gemini como fallback...');
      return await this.analyzeImageWithContext(imageUri, userMessage, language);

    } catch (error) {
      console.error('‚ùå [H√çBRIDO] Error en an√°lisis h√≠brido:', error);
      return VisionPrompts.getErrorMessage('visionAnalysis', language);
    }
  }

  // Funci√≥n combinada para an√°lisis de imagen + respuesta conversacional (Vision API)
  static async analyzeImageWithContext(imageUri: string, userMessage: string, language: SupportedLanguage = 'en'): Promise<string> {
    try {
      console.log('üñºÔ∏è [VISION-API] Iniciando analyzeImageWithContext...');
      console.log('üì± [VISION-API] Image URI:', imageUri);
      console.log('üí¨ [VISION-API] User message:', userMessage);
      
      // Primero analizar la imagen
      console.log('üîç [VISION-API] Analizando imagen...');
      const imageAnalysis = await this.getImageDescription(imageUri, language);
      console.log('üìä [VISION-API] An√°lisis de imagen completo:', imageAnalysis);
      
      // Crear mensaje de contexto usando VisionPrompts
      const contextMessage = `${VisionPrompts.getGeminiVisionPrompt(language, `El usuario envi√≥ una imagen de un plan/horario de trabajo y escribi√≥: "${userMessage}"`)}

INFORMACI√ìN EXTRA√çDA DE LA IMAGEN:
${imageAnalysis}

INSTRUCCIONES PARA EL AN√ÅLISIS:

IMPORTANTE: Analiza DIRECTAMENTE el texto extra√≠do de la imagen (secci√≥n "TEXTO DETECTADO").

${VisionPrompts.getMultiplePersonsDetection(language)}

${VisionPrompts.getSinglePersonAnalysis(language)}

${VisionPrompts.getResponseFormat(language)}

Si no hay texto legible, ind√≠calo claramente.`;

      console.log('üìù [IMAGEN] Context message generado:', contextMessage);
      console.log('üöÄ [IMAGEN] Enviando a Gemini para respuesta final...');
      
      const finalResponse = await this.getChatResponse(contextMessage, language);
      console.log('‚úÖ [IMAGEN] Respuesta final recibida:', finalResponse);
      
      return finalResponse;
    } catch (error) {
      console.error('Error en an√°lisis combinado:', error);
      return VisionPrompts.getErrorMessage('visionAnalysis', language);
    }
  }

  // Nueva funci√≥n para an√°lisis interactivo de horarios con detecci√≥n de ambig√ºedades
  static async analyzeWorkPlanInteractive(imageUri: string, userMessage: string, language: SupportedLanguage = 'es'): Promise<string> {
    try {
      console.log('ü§ñ [INTERACTIVO] Iniciando an√°lisis interactivo de horarios...');
      
      // API key ahora est√° en el servidor, no necesitamos verificarla aqu√≠

      // Convertir imagen a base64
      const base64Image = await FileSystem.readAsStringAsync(imageUri, {
        encoding: FileSystem.EncodingType.Base64,
      });

      // Crear prompt para detecci√≥n de ambig√ºedades
      const ambiguityPrompt = `${VisionPrompts.getPrompt('workScheduleAnalysisPrompt', language)}

${VisionPrompts.getPrompt('ambiguityDetectionPrompt', language)}

INSTRUCCIONES DE USUARIO: ${userMessage}

PASO 1: Examina esta imagen de horario de trabajo y detecta si hay elementos confusos o ambiguos.
PASO 2: Si hay ambig√ºedades, haz UNA pregunta espec√≠fica usando el formato de clarificationQuestionFormat.
PASO 3: Si todo est√° claro, procede con el an√°lisis normal usando el formato exacto especificado en responseFormat.

${VisionPrompts.getPrompt('responseFormat', language)}`;

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: ambiguityPrompt
              },
              {
                inline_data: {
                  mime_type: "image/jpeg",
                  data: base64Image
                }
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.3, // Menor temperatura para m√°s consistencia
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 512, // Respuesta m√°s corta para preguntas
        }
      };

      if (!this.GEMINI_API_KEY) {
        throw new Error('API key de Google Gemini no configurada');
      }

      const apiUrl = `${this.GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${this.GEMINI_API_KEY}`;
      console.log('üì§ [INTERACTIVO] Enviando request directo a Google Gemini API...');
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå [INTERACTIVO] Error:', errorData);
        throw new Error(`Error en an√°lisis interactivo: ${response.status} - ${errorData.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      console.log('üì• [INTERACTIVO] Response data:', JSON.stringify(data, null, 2));

      if (data.candidates && data.candidates.length > 0) {
        const candidate = data.candidates[0];
        if (candidate.content && candidate.content.parts && candidate.content.parts.length > 0) {
          const responseText = candidate.content.parts[0].text;
          console.log('‚úÖ [INTERACTIVO] Respuesta interactiva:', responseText);
          
          // Verificar si la respuesta contiene una pregunta de clarificaci√≥n
          if (responseText.includes('ü§î') || responseText.toLowerCase().includes('necesito aclarar') || 
              responseText.toLowerCase().includes('qu√© significa') || responseText.toLowerCase().includes('antes de continuar')) {
            console.log('‚ùì [INTERACTIVO] Se detect√≥ pregunta de clarificaci√≥n');
            return responseText;
          }
          
          // Verificar si la respuesta tiene el formato correcto para el parser
          if (responseText.includes('üë§ **PERSONA:**') && responseText.includes('üìÖ **D√çAS DE TRABAJO:**')) {
            console.log('‚úÖ [INTERACTIVO] Respuesta con formato correcto detectada');
            return responseText;
          }
          
          // Si no es pregunta ni tiene formato correcto, hacer fallback
          console.log('‚ö†Ô∏è [INTERACTIVO] Respuesta sin formato correcto, usando fallback');
          throw new Error('Formato incorrecto, usando fallback');
        }
      }

      console.log('‚ö†Ô∏è [INTERACTIVO] Respuesta vac√≠a, usando fallback');
      return VisionPrompts.getErrorMessage('visionAnalysis', language);

    } catch (error) {
      console.error('‚ùå [INTERACTIVO] Error en an√°lisis interactivo:', error);
      throw error; // Re-throw para que el m√©todo padre use fallback
    }
  }
}